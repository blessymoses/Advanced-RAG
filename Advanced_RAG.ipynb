{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aishwaryaprabhat/Advanced-RAG/blob/main/Advanced_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhMdMY9Ly4lk"
      },
      "source": [
        "# Data Download and Environment Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "If running on Google Colab, first clone the repo and then change directory:\n",
        "```\n",
        "!git clone https://github.com/aishwaryaprabhat/Advanced-RAG/\n",
        "%cd Advanced-RAG\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1704857605192
        },
        "id": "vOEmK9Nwyj7R",
        "outputId": "a79f7c5e-5bfb-44c2-da0d-c0989edcc0fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'DataRepository'...\n",
            "remote: Enumerating objects: 114, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 114 (delta 35), reused 34 (delta 9), pack-reused 8 (from 1)\u001b[K\n",
            "Receiving objects: 100% (114/114), 77.61 MiB | 15.94 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "Archive:  DataRepository/high-performance-rag/Camel Papers Test.zip\n",
            "  inflating: source_docs/Acute respiratory distress syndrome in an alpaca cria.pdf  \n",
            "  inflating: source_docs/Alpaca liveweight variations and fiber production in Mediterranean range of Chile.pdf  \n",
            "Archive:  DataRepository/high-performance-rag/Camel Papers Train.zip\n",
            "  inflating: source_docs/Antibody response to the epsilon toxin ofClostridium perfringensfollowing vaccination of Lama glamacrias.pdf  \n",
            "  inflating: source_docs/Comparative pigmentation of sheep, goats, and llamas what colors are possible through selection.pdf  \n",
            "  inflating: source_docs/Conservative management of a ruptured.pdf  \n",
            "  inflating: source_docs/Evaluation of cholesterol and vitamin E concentrations in adult alpacas and nursing crias.pdf  \n",
            "  inflating: source_docs/Influence of effects on quality traits and relationships between traits of the llama fleece..pdf  \n",
            "  inflating: source_docs/Influence of Follicular Fluid on in Vitro.pdf  \n",
            "  inflating: source_docs/Neurological Causes of Diaphragmatic Paralysis in 11 Alpacas.pdf  \n",
            "  inflating: source_docs/On the morphology of the cerebellum of the alpaca (Lama pacos)..pdf  \n",
            "  inflating: source_docs/Relationships between integumental characteristics and.pdf  \n",
            "  inflating: source_docs/Respiratory mechanics and results of cytologic examination of bronchoalveolar lavage fluid in healthy adult alpacas.pdf  \n",
            "  inflating: source_docs/Serum and urine analyte comparison between llamas and alpacas fed three forages.pdf  \n",
            "  inflating: source_docs/The physiological impact of wool-harvesting procedures in vicunas (Vicugna vicugna)..pdf  \n"
          ]
        }
      ],
      "source": [
        "!bash download_dataset.sh # get from https://github.com/aishwaryaprabhat/Advanced-RAG/blob/main/download_dataset.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1705046698440
        },
        "id": "qoNQu1bszvmI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting llama-index==0.9.27\n",
            "  Downloading llama_index-0.9.27-py3-none-any.whl (15.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.8 MB 4.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pypdf==3.17.4\n",
            "  Downloading pypdf-3.17.4-py3-none-any.whl (278 kB)\n",
            "\u001b[K     |████████████████████████████████| 278 kB 37.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting sentence-transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 28.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting typing_extensions==4.7.1\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting nest_asyncio==1.5.8\n",
            "  Downloading nest_asyncio-1.5.8-py3-none-any.whl (5.3 kB)\n",
            "Collecting ragas==0.0.22\n",
            "  Downloading ragas-0.0.22-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 5.7 MB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting SQLAlchemy[asyncio]>=1.4.49\n",
            "  Downloading SQLAlchemy-2.0.36-cp39-cp39-macosx_11_0_arm64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 22.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting nltk<4.0.0,>=3.8.1\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 18.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 53.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting requests>=2.31.0\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 19.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting tiktoken>=0.3.3\n",
            "  Downloading tiktoken-0.8.0-cp39-cp39-macosx_11_0_arm64.whl (983 kB)\n",
            "\u001b[K     |████████████████████████████████| 983 kB 15.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting openai>=1.1.0\n",
            "  Downloading openai-1.54.0-py3-none-any.whl (389 kB)\n",
            "\u001b[K     |████████████████████████████████| 389 kB 79.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting deprecated>=1.2.9.3\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 31.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting networkx>=3.0\n",
            "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 10.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting dataclasses-json\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting aiohttp<4.0.0,>=3.8.6\n",
            "  Downloading aiohttp-3.10.10-cp39-cp39-macosx_11_0_arm64.whl (391 kB)\n",
            "\u001b[K     |████████████████████████████████| 391 kB 32.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting typing-inspect>=0.8.0\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting beautifulsoup4<5.0.0,>=4.12.2\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 84.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting fsspec>=2023.5.0\n",
            "  Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
            "\u001b[K     |████████████████████████████████| 179 kB 89.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting tenacity<9.0.0,>=8.2.0\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 84.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0 MB 10.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 29.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting torch>=1.6.0\n",
            "  Downloading torch-2.5.1-cp39-none-macosx_11_0_arm64.whl (63.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 63.9 MB 504 kB/s eta 0:00:011\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading torchvision-0.20.1-cp39-cp39-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 14.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.5.2-cp39-cp39-macosx_12_0_arm64.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 63.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 30.3 MB 27.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.2.0-cp39-cp39-macosx_11_0_arm64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 88.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[K     |████████████████████████████████| 447 kB 41.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pysbd>=0.3.4\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[K     |████████████████████████████████| 71 kB 32.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 83.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[K     |████████████████████████████████| 480 kB 86.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.12.0\n",
            "  Downloading yarl-1.17.1-cp39-cp39-macosx_11_0_arm64.whl (91 kB)\n",
            "\u001b[K     |████████████████████████████████| 91 kB 45.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.5.0-cp39-cp39-macosx_11_0_arm64.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 9.7 MB/s  eta 0:00:01\n",
            "\u001b[?25hCollecting attrs>=17.3.0\n",
            "  Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 11.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting aiohappyeyeballs>=2.3.0\n",
            "  Downloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.1.0-cp39-cp39-macosx_11_0_arm64.whl (29 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Collecting wrapt<2,>=1.10\n",
            "  Downloading wrapt-1.16.0-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
            "Collecting filelock\n",
            "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: packaging>=20.9 in /Users/blessy/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2->-r requirements.txt (line 3)) (24.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
            "\u001b[K     |████████████████████████████████| 172 kB 79.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting regex>=2021.8.3\n",
            "  Downloading regex-2024.9.11-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 44.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 45.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "\u001b[K     |████████████████████████████████| 301 kB 43.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting anyio<5,>=3.5.0\n",
            "  Downloading anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 40.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting distro<2,>=1.7.0\n",
            "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Collecting openai>=1.1.0\n",
            "  Downloading openai-1.53.1-py3-none-any.whl (387 kB)\n",
            "\u001b[K     |████████████████████████████████| 387 kB 61.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.53.0-py3-none-any.whl (387 kB)\n",
            "\u001b[K     |████████████████████████████████| 387 kB 36.0 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.52.2-py3-none-any.whl (386 kB)\n",
            "\u001b[K     |████████████████████████████████| 386 kB 46.6 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.52.1-py3-none-any.whl (386 kB)\n",
            "\u001b[K     |████████████████████████████████| 386 kB 71.6 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.52.0-py3-none-any.whl (386 kB)\n",
            "\u001b[K     |████████████████████████████████| 386 kB 74.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "\u001b[K     |████████████████████████████████| 383 kB 54.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.51.1-py3-none-any.whl (383 kB)\n",
            "\u001b[K     |████████████████████████████████| 383 kB 64.7 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.51.0-py3-none-any.whl (383 kB)\n",
            "\u001b[K     |████████████████████████████████| 383 kB 78.5 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.50.2-py3-none-any.whl (382 kB)\n",
            "\u001b[K     |████████████████████████████████| 382 kB 82.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.50.1-py3-none-any.whl (378 kB)\n",
            "\u001b[K     |████████████████████████████████| 378 kB 13.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.50.0-py3-none-any.whl (378 kB)\n",
            "\u001b[K     |████████████████████████████████| 378 kB 50.6 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.49.0-py3-none-any.whl (378 kB)\n",
            "\u001b[K     |████████████████████████████████| 378 kB 24.1 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.48.0-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 78.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.47.1-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 76.0 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.47.0-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 80.5 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.46.1-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 80.3 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.46.0-py3-none-any.whl (375 kB)\n",
            "\u001b[K     |████████████████████████████████| 375 kB 58.8 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.45.1-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 41.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.45.0-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 85.0 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.44.1-py3-none-any.whl (373 kB)\n",
            "\u001b[K     |████████████████████████████████| 373 kB 18.8 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.44.0-py3-none-any.whl (367 kB)\n",
            "\u001b[K     |████████████████████████████████| 367 kB 18.6 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.43.1-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 30.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.43.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 45.8 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[K     |████████████████████████████████| 362 kB 59.9 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.41.1-py3-none-any.whl (362 kB)\n",
            "\u001b[K     |████████████████████████████████| 362 kB 78.9 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.41.0-py3-none-any.whl (362 kB)\n",
            "\u001b[K     |████████████████████████████████| 362 kB 58.3 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.40.8-py3-none-any.whl (361 kB)\n",
            "\u001b[K     |████████████████████████████████| 361 kB 27.3 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.40.7-py3-none-any.whl (361 kB)\n",
            "\u001b[K     |████████████████████████████████| 361 kB 39.0 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.40.6-py3-none-any.whl (361 kB)\n",
            "\u001b[K     |████████████████████████████████| 361 kB 25.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.40.5-py3-none-any.whl (361 kB)\n",
            "\u001b[K     |████████████████████████████████| 361 kB 72.7 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.40.4-py3-none-any.whl (361 kB)\n",
            "\u001b[K     |████████████████████████████████| 361 kB 76.9 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.40.3-py3-none-any.whl (360 kB)\n",
            "\u001b[K     |████████████████████████████████| 360 kB 42.6 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.40.2-py3-none-any.whl (360 kB)\n",
            "\u001b[K     |████████████████████████████████| 360 kB 28.8 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.40.1-py3-none-any.whl (360 kB)\n",
            "\u001b[K     |████████████████████████████████| 360 kB 54.3 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.40.0-py3-none-any.whl (360 kB)\n",
            "\u001b[K     |████████████████████████████████| 360 kB 41.7 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading openai-1.39.0-py3-none-any.whl (336 kB)\n",
            "\u001b[K     |████████████████████████████████| 336 kB 42.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pydantic<3,>=1.9.0\n",
            "  Downloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
            "\u001b[K     |████████████████████████████████| 434 kB 83.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: exceptiongroup>=1.0.2 in /Users/blessy/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai>=1.1.0->llama-index==0.9.27->-r requirements.txt (line 1)) (1.2.2)\n",
            "Collecting idna>=2.8\n",
            "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 37.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting certifi\n",
            "  Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
            "\u001b[K     |████████████████████████████████| 167 kB 29.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting httpcore==1.*\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 31.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 30.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pydantic-core==2.23.4\n",
            "  Downloading pydantic_core-2.23.4-cp39-cp39-macosx_11_0_arm64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 85.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting annotated-types>=0.6.0\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 32.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.4.0-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 38.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting greenlet!=0.4.17\n",
            "  Downloading greenlet-3.1.1-cp39-cp39-macosx_11_0_universal2.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 79.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting sympy==1.13.1\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 71.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting torch>=1.6.0\n",
            "  Downloading torch-2.5.0-cp39-none-macosx_11_0_arm64.whl (64.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 64.3 MB 33.2 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading torch-2.4.1-cp39-none-macosx_11_0_arm64.whl (62.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 62.1 MB 9.4 MB/s eta 0:00:011\n",
            "\u001b[?25h  Downloading torch-2.4.0-cp39-none-macosx_11_0_arm64.whl (62.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 62.1 MB 28.8 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading torch-2.3.1-cp39-none-macosx_11_0_arm64.whl (61.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 61.0 MB 3.3 MB/s eta 0:00:011\n",
            "\u001b[?25h  Downloading torch-2.3.0-cp39-none-macosx_11_0_arm64.whl (61.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 61.0 MB 114.9 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading torch-2.2.2-cp39-none-macosx_11_0_arm64.whl (59.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.7 MB 81.0 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading torch-2.2.1-cp39-none-macosx_11_0_arm64.whl (59.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.7 MB 219 kB/s eta 0:00:011\n",
            "\u001b[?25h  Downloading torch-2.2.0-cp39-none-macosx_11_0_arm64.whl (59.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.7 MB 4.3 MB/s eta 0:00:012\n",
            "\u001b[?25h  Downloading torch-2.1.2-cp39-none-macosx_11_0_arm64.whl (59.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.6 MB 20.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting sympy\n",
            "  Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 84.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 63.7 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting safetensors>=0.4.1\n",
            "  Downloading safetensors-0.4.5-cp39-cp39-macosx_11_0_arm64.whl (383 kB)\n",
            "\u001b[K     |████████████████████████████████| 383 kB 52.0 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting tokenizers<0.21,>=0.20\n",
            "  Downloading tokenizers-0.20.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 68.6 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting propcache>=0.2.0\n",
            "  Downloading propcache-0.2.0-cp39-cp39-macosx_11_0_arm64.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 20.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 29.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting multiprocess<0.70.17\n",
            "  Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 56.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pyarrow>=15.0.0\n",
            "  Downloading pyarrow-18.0.0-cp39-cp39-macosx_12_0_arm64.whl (29.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.5 MB 23.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 62.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
            "Collecting fsspec[http]<=2024.9.0,>=2023.1.0\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[K     |████████████████████████████████| 179 kB 81.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0\n",
            "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17\n",
            "  Downloading langsmith-0.1.139-py3-none-any.whl (302 kB)\n",
            "\u001b[K     |████████████████████████████████| 302 kB 77.4 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting langchain-core<0.4.0,>=0.3.15\n",
            "  Downloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
            "\u001b[K     |████████████████████████████████| 408 kB 69.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.0 MB 43.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting jsonpatch<2.0,>=1.33\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14\n",
            "  Downloading orjson-3.10.11-cp39-cp39-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (266 kB)\n",
            "\u001b[K     |████████████████████████████████| 266 kB 20.2 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 17.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/blessy/Library/Python/3.9/lib/python/site-packages (from pandas->llama-index==0.9.27->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[K     |████████████████████████████████| 508 kB 20.5 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting tzdata>=2022.7\n",
            "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "\u001b[K     |████████████████████████████████| 346 kB 71.8 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->llama-index==0.9.27->-r requirements.txt (line 1)) (1.15.0)\n",
            "Collecting threadpoolctl>=3.1.0\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[K     |████████████████████████████████| 536 kB 75.8 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
            "  Downloading pillow-11.0.0-cp39-cp39-macosx_11_0_arm64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 84.1 MB/s eta 0:00:01\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.20.0-cp39-cp39-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 18.2 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading torchvision-0.19.1-cp39-cp39-macosx_11_0_arm64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 78.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading torchvision-0.19.0-cp39-cp39-macosx_11_0_arm64.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 78.6 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading torchvision-0.18.1-cp39-cp39-macosx_11_0_arm64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 16.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading torchvision-0.18.0-cp39-cp39-macosx_11_0_arm64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 22.4 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading torchvision-0.17.2-cp39-cp39-macosx_11_0_arm64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 22.7 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading torchvision-0.17.1-cp39-cp39-macosx_11_0_arm64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 20.1 MB/s eta 0:00:01\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading torchvision-0.17.0-cp39-cp39-macosx_11_0_arm64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 24.7 MB/s eta 0:00:01\n",
            "\u001b[?25h  Downloading torchvision-0.16.2-cp39-cp39-macosx_11_0_arm64.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 17.0 MB/s eta 0:00:01\n",
            "\u001b[?25hBuilding wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=245e634c7af9eb8b7bfb0bb7b7fca568d22deb3ffd6b53cd8b02420d5f01641e\n",
            "  Stored in directory: /Users/blessy/Library/Caches/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: urllib3, typing-extensions, sniffio, idna, h11, charset-normalizer, certifi, requests, pydantic-core, httpcore, anyio, annotated-types, requests-toolbelt, pydantic, propcache, orjson, multidict, jsonpointer, httpx, frozenlist, yarl, tqdm, tenacity, pyyaml, mpmath, MarkupSafe, langsmith, jsonpatch, fsspec, filelock, attrs, async-timeout, aiosignal, aiohappyeyeballs, tzdata, sympy, pytz, numpy, networkx, mypy-extensions, langchain-core, jinja2, huggingface-hub, dill, aiohttp, xxhash, wrapt, typing-inspect, torch, tokenizers, threadpoolctl, SQLAlchemy, soupsieve, scipy, safetensors, regex, pyarrow, pillow, pandas, multiprocess, marshmallow, langchain-text-splitters, joblib, greenlet, distro, click, transformers, torchvision, tiktoken, sentencepiece, scikit-learn, pysbd, openai, nltk, nest-asyncio, langchain, deprecated, datasets, dataclasses-json, beautifulsoup4, sentence-transformers, ragas, pypdf, llama-index\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.12.2\n",
            "    Uninstalling typing-extensions-4.12.2:\n",
            "      Successfully uninstalled typing-extensions-4.12.2\n",
            "  Attempting uninstall: nest-asyncio\n",
            "    Found existing installation: nest-asyncio 1.6.0\n",
            "    Uninstalling nest-asyncio-1.6.0:\n",
            "      Successfully uninstalled nest-asyncio-1.6.0\n",
            "Successfully installed MarkupSafe-3.0.2 SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.2.post1 async-timeout-4.0.3 attrs-24.2.0 beautifulsoup4-4.12.3 certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 dataclasses-json-0.6.7 datasets-3.1.0 deprecated-1.2.14 dill-0.3.8 distro-1.9.0 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.26.2 idna-3.10 jinja2-3.1.4 joblib-1.4.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.7 langchain-core-0.3.15 langchain-text-splitters-0.3.2 langsmith-0.1.139 llama-index-0.9.27 marshmallow-3.23.1 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 mypy-extensions-1.0.0 nest-asyncio-1.5.8 networkx-3.2.1 nltk-3.9.1 numpy-1.26.4 openai-1.39.0 orjson-3.10.11 pandas-2.2.3 pillow-11.0.0 propcache-0.2.0 pyarrow-18.0.0 pydantic-2.9.2 pydantic-core-2.23.4 pypdf-3.17.4 pysbd-0.3.4 pytz-2024.2 pyyaml-6.0.2 ragas-0.0.22 regex-2024.9.11 requests-2.32.3 requests-toolbelt-1.0.0 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.13.1 sentence-transformers-2.2.2 sentencepiece-0.2.0 sniffio-1.3.1 soupsieve-2.6 sympy-1.13.3 tenacity-8.5.0 threadpoolctl-3.5.0 tiktoken-0.8.0 tokenizers-0.20.2 torch-2.1.2 torchvision-0.16.2 tqdm-4.66.6 transformers-4.46.1 typing-extensions-4.7.1 typing-inspect-0.9.0 tzdata-2024.2 urllib3-2.2.3 wrapt-1.16.0 xxhash-3.5.0 yarl-1.17.1\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
            "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# %pip install llama-index pypdf sentence_transformers typing_extensions==4.7.1 nest_asyncio -U -q\n",
        "%pip install --upgrade -r requirements.txt -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1705155980271
        },
        "id": "T36WDvn30Pd1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-*'\n",
        "os.environ['HUGGINGFACE_API_TOKEN'] = 'hf_*'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1705156028902
        },
        "id": "0td9HhX66XTw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/jupyter_env/lib/python3.8/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c5b3eb83c7e41649b46dd6e07493184",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20568cb42b3646158dbe5de539416da1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c77923d7930340a48f4ce71e6f8c85d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cd6a57db18948f6b8cddb649f2ec411",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d0cb22dc59840a5a5b21f87eec08e66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73033d9c23d14e6a8389156d89921922",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "# Initialize an embedding model from Hugging Face using the \"BAAI/bge-small-en\" model.\n",
        "embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
        "\n",
        "# Create an OpenAI GPT-3.5 model instance with no randomness in responses (temperature=0).\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "# Load data from a directory named 'source_docs' using SimpleDirectoryReader.\n",
        "source_docs = SimpleDirectoryReader('source_docs').load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ5GJYC_0t2o"
      },
      "source": [
        "# Advanced RAG Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNHwqcmj0zQv"
      },
      "source": [
        "## Chunking with Overlap (Baseline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpL_9Bx05Cch"
      },
      "source": [
        "### Parse source_docs into nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1705024279454
        },
        "id": "BMBcOP_C0VZJ"
      },
      "outputs": [],
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "\n",
        "# Create an instance of SimpleNodeParser with default settings.\n",
        "# Parameters:\n",
        "#   chunk_overlap: Specifies the number of overlapping characters between adjacent text chunks.\n",
        "#                  This is useful for ensuring that context isn't lost at the boundaries of chunks.\n",
        "#   chunk_size:    Defines the size of each text chunk in characters. \n",
        "#                  This determines how the text is segmented for processing.\n",
        "baseline_parser = SimpleNodeParser.from_defaults(\n",
        "    chunk_overlap=200,  # Overlap of 200 characters between chunks\n",
        "    chunk_size=1024     # Each chunk consists of 1024 characters\n",
        ")\n",
        "\n",
        "# Use the created parser instance to extract nodes from the documents.\n",
        "# The 'get_nodes_from_documents' method processes the documents in 'source_docs'\n",
        "# and extracts structured nodes based on the parser's configuration.\n",
        "baseline_nodes = baseline_parser.get_nodes_from_documents(source_docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1705024347773
        },
        "id": "SyCNfQ2a6tP-"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "from llama_index import ServiceContext\n",
        "\n",
        "# Create a ServiceContext instance with default settings.\n",
        "# Parameters:\n",
        "#   llm: The language model to be used within the service context. \n",
        "#        This defines how text will be interpreted or processed.\n",
        "#   embed_model: The embedding model used for converting text to numerical representations,\n",
        "#                facilitating operations like similarity search or clustering.\n",
        "#   node_parser: The node parser that structures and segments text data into manageable parts.\n",
        "#                It was defined previously in your code.\n",
        "baseline_context = ServiceContext.from_defaults(\n",
        "    llm=llm,                # Language model\n",
        "    embed_model=embedding_model,  # Embedding model\n",
        "    node_parser=baseline_parser   # Node parser from previous code\n",
        ")\n",
        "\n",
        "# Initialize a VectorStoreIndex with the baseline nodes and the service context.\n",
        "# The VectorStoreIndex is used for efficient storage and retrieval of vectorized text data.\n",
        "# Parameters:\n",
        "#   baseline_nodes: The nodes extracted from documents, ready for indexing.\n",
        "#   service_context: The service context that provides essential components like the embedding model.\n",
        "baseline_index = VectorStoreIndex(\n",
        "    baseline_nodes,              # Nodes to be indexed\n",
        "    service_context=baseline_context  # Service context providing necessary components\n",
        ")\n",
        "\n",
        "# Persist the baseline index in a specified directory.\n",
        "# This step saves the state of the index to disk, allowing for later retrieval or backup.\n",
        "# Parameters:\n",
        "#   persist_dir: The directory name where the index will be stored.\n",
        "baseline_index.storage_context.persist(\n",
        "    persist_dir=\"baseline_index\"  # Directory name for storing the index\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "gather": {
          "logged": 1705025880418
        },
        "id": "0U_1fJxH7w_W",
        "outputId": "d36607b4-8d2e-4ac2-e2b6-25e783305d32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Camelid genetics can influence wool quality. The inheritance of coat colors in alpacas and llamas, which are types of camelids, has been studied. Additionally, major genes affecting alpaca fiber traits have been analyzed. The expression patterns of keratin intermediate filament and keratin associated protein genes in wool follicles have also been investigated. These studies suggest that genetic factors play a role in determining the quality of wool in camelids.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the baseline index into a query engine capable of finding the top 3 most similar entries.\n",
        "baseline_query_engine = baseline_index.as_query_engine(similarity_top_k=3)\n",
        "\n",
        "# Perform a query with the baseline query engine asking about the influence of camelid genetics on wool quality.\n",
        "baseline_response = baseline_query_engine.query(\"How do camelid genetics influence wool quality?\")\n",
        "\n",
        "# Retrieve the response from the query.\n",
        "baseline_response.response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFrYk5bw9Yxm"
      },
      "source": [
        "## Sentence Window Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1705156070397
        },
        "id": "ecV4IU8r9IHu",
        "outputId": "204d7188-d2bc-4f72-a0dd-7323fcd21fe6"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.node_parser import SentenceWindowNodeParser\n",
        "\n",
        "# Initialize a SentenceWindowNodeParser with specific settings.\n",
        "# A SentenceWindowNodeParser is designed to parse and structure text data into nodes,\n",
        "# with a focus on sentence-level granularity.\n",
        "# Parameters:\n",
        "#   window_size: Defines the number of sentences to include in each window or node. \n",
        "#                This sets the scope of context for each node.\n",
        "#   window_metadata_key: The key under which window metadata (like window number) will be stored.\n",
        "#   original_text_metadata_key: The key for storing the original text data in the metadata.\n",
        "sentence_parser = SentenceWindowNodeParser.from_defaults(\n",
        "    window_size=6,  # Number of sentences in each window\n",
        "    window_metadata_key=\"window\",  # Metadata key for window information\n",
        "    original_text_metadata_key=\"original_text\"  # Metadata key for original text\n",
        ")\n",
        "\n",
        "# Use the sentence parser to parse nodes from documents.\n",
        "# This method processes the documents in 'source_docs' and extracts structured nodes,\n",
        "# with each node representing a window of sentences.\n",
        "sentence_nodes = sentence_parser.get_nodes_from_documents(source_docs)\n",
        "\n",
        "# Create a ServiceContext using the sentence parser and previously defined models.\n",
        "# This context will use the sentence-level node parser for processing text data,\n",
        "# along with the specified language and embedding models.\n",
        "# Parameters:\n",
        "#   llm: The language model for the service context.\n",
        "#   embed_model: The embedding model for converting text to numerical representations.\n",
        "#   node_parser: The sentence-level node parser for structuring text data.\n",
        "sentence_context = ServiceContext.from_defaults(\n",
        "    llm=llm,  # Language model\n",
        "    embed_model=embedding_model,  # Embedding model\n",
        "    node_parser=sentence_parser  # Sentence-level node parser\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1705156512906
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node ID: 95839096-8c3e-49d9-ba84-23d39459a661\n",
            "Text: Case description An approximately  7-hour-old 5.2-kg female\n",
            "intact Suri alpaca  cria was presented to the Oklahoma State\n",
            "University Boren  Veterinary Medical Teaching Hospital with the\n",
            "complaint of  being hypothermic, lethargic, and unable to stand and\n",
            "nurse. \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'window': '(Traduit par Isabelle Vallières)\\nCan Vet J 2011;52:784–787\\nIntroduction\\nAcute respiratory distress syndrome, referred to as ARDS, \\nis the manifestation of an intra- or extra-pulmonary insult \\nresulting in an overzealous inflammatory cascade in the lungs. \\n Ultimately, interstitial pulmonary edema develops and is fre -\\nquently fatal.  The syndrome was first described in humans but \\nhas since been recognized in animals, particularly in companion animals and foals (1,2).  Mortality rates vary from up to 60% in humans to almost 100% in small animal species (2).  This syn -\\ndrome has not been previously reported in a camelid species, but should be included on the list of differential diagnoses for crias with acute onset respiratory distress.  Although the prognosis in other species is often guarded, treatment of alpaca crias can \\nhave a good outcome.\\n Case description\\nAn approximately  7-hour-old 5.2-kg female intact Suri alpaca \\ncria was presented to the Oklahoma State University Boren \\nVeterinary Medical Teaching Hospital with the complaint of \\nbeing hypothermic, lethargic, and unable to stand and nurse. \\n By the owners’ records the cria was at least 1 wk premature.  The primiparous dam had delivered the cria unassisted and unobserved.  Upon finding the cria, the owners determined that she was hypothermic (actual temperature not reported) and attempted to warm her with blankets, a heater, and warm water baths.  Attempts were made to milk out the dam, but only 5 mL of colostrum were obtained and were fed to the cria along with \\n60\\xa0mL of milk replacer, which the cria suckled readily from a \\nbottle.  However, lethargy and inability to rise persisted and the cria was admitted to the hospital the following morning.\\n',\n",
              " 'original_text': 'Case description\\nAn approximately  7-hour-old 5.2-kg female intact Suri alpaca \\ncria was presented to the Oklahoma State University Boren \\nVeterinary Medical Teaching Hospital with the complaint of \\nbeing hypothermic, lethargic, and unable to stand and nurse. \\n',\n",
              " 'page_label': '784',\n",
              " 'file_name': 'Acute respiratory distress syndrome in an alpaca cria.pdf',\n",
              " 'file_path': 'source_docs/Acute respiratory distress syndrome in an alpaca cria.pdf',\n",
              " 'file_type': 'application/pdf',\n",
              " 'file_size': 501704,\n",
              " 'creation_date': '2024-01-10',\n",
              " 'last_modified_date': '2023-10-09',\n",
              " 'last_accessed_date': '2023-10-09'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(sentence_nodes[15], \"\\n\")\n",
        "sentence_nodes[15].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1705024847805
        },
        "id": "1DhpZMCDuObv"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "# Create a VectorStoreIndex using the parsed sentence nodes and the previously defined service context.\n",
        "# VectorStoreIndex is a structure used for efficient storage, retrieval, and manipulation of vectorized text data.\n",
        "# Parameters:\n",
        "#   sentence_nodes: The nodes obtained from parsing the documents at the sentence level.\n",
        "#                   These nodes are now ready for indexing.\n",
        "#   service_context: The service context that provides essential components (like models and parsers) for the indexing.\n",
        "sentence_index = VectorStoreIndex(\n",
        "    sentence_nodes,         # Nodes obtained from sentence-level parsing\n",
        "    service_context=sentence_context  # Service context with essential components\n",
        ")\n",
        "\n",
        "# Persist the sentence index to a directory. This step saves the current state of the index on disk.\n",
        "# It allows for the index to be reloaded and used in the future, ensuring data persistence.\n",
        "# Parameters:\n",
        "#   persist_dir: The name of the directory where the index will be stored.\n",
        "sentence_index.storage_context.persist(\n",
        "    persist_dir=\"sentence_index\"  # Directory name for storing the index\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "gather": {
          "logged": 1705025864395
        },
        "id": "gkEPhxcAusIr",
        "outputId": "d8a017b8-8aee-4d49-cd9e-283889554c97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Camelid genetics influence wool quality through various mechanisms. One important aspect is coat color genetics, where llamas and alpacas exhibit a wide range of natural colors and patterns. Llamas, in particular, have greater color variation compared to alpacas. This variation is attributed to the selection process during domestication, where llamas were primarily selected for body size and fiber weight rather than color uniformity or fiber fineness. \\n\\nAdditionally, the composition and interactions of keratin intermediate filaments (KIFs) and keratin-associated proteins (KAPs) play a crucial role in determining fiber characteristics. Fiber growth in mammals, including camelids, is a cyclical process regulated by genetics, nutrition, and hormones. The proteins that form the fiber are encoded by keratin genes (KRT) and keratin-associated proteins (KRTAP), which are expressed in a highly regulated manner during hair follicle growth.\\n\\nGenetic selection programs have been implemented to improve fleece characteristics in domestic camelids. However, the genetic mechanisms controlling fiber traits in llamas and alpacas are not fully understood. Studies have identified major genes that affect quantitative fiber traits such as fiber diameter, standard deviation of fiber diameter, variation coefficient, and comfort factor. Molecular identification of these genes would facilitate genetic improvement.\\n\\nFurthermore, genes such as fibroblast growth factor 5 (FGF5) play a role in regulating hair follicle growth and affecting hair length. Alternative splicing of the FGF5 gene results in different transcripts that control the catagen and anagen phases of hair growth. Loss-of-function mutations in the FGF5 gene have been associated with long-hair phenotypes in camelids. Differences in FGF5 expression have also been observed between different regions of the body and between different types of alpacas.\\n\\nOverall, camelid genetics influence wool quality through factors such as coat color variation, the composition of keratin proteins, and the regulation of hair follicle growth.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
        "\n",
        "# Convert the sentence index into a query engine.\n",
        "# The query engine is configured to find the top 3 most similar entries in the index\n",
        "# when performing a query. This is useful for retrieving the most relevant information\n",
        "# based on a given input.\n",
        "# Parameters:\n",
        "#   similarity_top_k: The number of top similar entries to retrieve. Here, it's set to 3.\n",
        "#   node_postprocessors: A list of postprocessors to apply on the nodes. In this case,\n",
        "#                        a MetadataReplacementPostProcessor is used, which replaces the node's\n",
        "#                        metadata with values from the 'window' key. This can be helpful for\n",
        "#                        contextualizing results based on specific metadata.\n",
        "sentence_query_engine = sentence_index.as_query_engine(\n",
        "    similarity_top_k=3,  # Find the top 3 most similar entries\n",
        "    node_postprocessors=[\n",
        "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Perform a query using the sentence query engine.\n",
        "# This query is about the influence of camelid genetics on wool quality. The query engine will\n",
        "# process this input and find the most relevant entries in the index that match the query.\n",
        "sentence_response = sentence_query_engine.query(\"How do camelid genetics influence wool quality?\")\n",
        "\n",
        "# Retrieve the response from the query.\n",
        "# The response contains the top similar entries as determined by the query engine,\n",
        "# potentially providing insightful information about the query topic.\n",
        "response = sentence_response.response\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J_pXwebwgbI"
      },
      "source": [
        "## Automerging Retrival (Using Hierarchical Nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1705025020941
        },
        "id": "aRMlnoTHvS3B"
      },
      "outputs": [],
      "source": [
        "from llama_index.node_parser import HierarchicalNodeParser\n",
        "\n",
        "# Initialize a HierarchicalNodeParser with default settings.\n",
        "# HierarchicalNodeParser is designed to parse and structure text data into a hierarchy of nodes,\n",
        "# allowing for a more structured and layered representation of the text.\n",
        "# This can be particularly useful for complex documents where different levels of granularity are needed.\n",
        "hierarchical_parser = HierarchicalNodeParser.from_defaults()\n",
        "# No additional parameters are needed for default settings.\n",
        "\n",
        "# Parse nodes from the documents using the hierarchical parser.\n",
        "# This method processes the documents in 'source_docs' and extracts structured nodes,\n",
        "# organizing them hierarchically based on the document structure.\n",
        "hierarchical_nodes = hierarchical_parser.get_nodes_from_documents(source_docs)\n",
        "\n",
        "# Create a ServiceContext using the hierarchical parser and previously defined models.\n",
        "# This context will now use the hierarchical node parser for processing text data,\n",
        "# in conjunction with the specified language and embedding models.\n",
        "# Parameters:\n",
        "#   llm: The language model for the service context, used for understanding and processing language data.\n",
        "#   embed_model: The embedding model for converting text into numerical representations,\n",
        "#                enabling various text analysis tasks.\n",
        "#   node_parser: The hierarchical node parser for structuring the text data.\n",
        "hierarchical_context = ServiceContext.from_defaults(\n",
        "    llm=llm,  # Language model\n",
        "    embed_model=embedding_model,  # Embedding model\n",
        "    node_parser=hierarchical_parser  # Hierarchical node parser\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1705025330992
        },
        "id": "-bzEmN0ZxoSq"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex, StorageContext\n",
        "\n",
        "# Create a VectorStoreIndex using the parsed hierarchical nodes and the specified service context.\n",
        "# VectorStoreIndex is used for efficient storage, retrieval, and manipulation of vectorized text data.\n",
        "# Parameters:\n",
        "#   hierarchical_nodes: The nodes obtained from parsing the documents using the hierarchical node parser.\n",
        "#                       These nodes represent the text data structured in a hierarchical format.\n",
        "#   service_context: The service context that includes essential components like models and parsers\n",
        "#                    for processing and understanding the text data.\n",
        "hierarchical_index = VectorStoreIndex(\n",
        "    hierarchical_nodes,           # Nodes structured hierarchically\n",
        "    service_context=hierarchical_context  # Service context with essential components\n",
        ")\n",
        "\n",
        "# Persist the hierarchical index to a directory. This action saves the current state of the index on disk,\n",
        "# enabling the index to be reloaded and reused in the future. It ensures the persistence and availability\n",
        "# of the indexed data for later use.\n",
        "# Parameters:\n",
        "#   persist_dir: The name of the directory where the index will be stored.\n",
        "hierarchical_index.storage_context.persist(\n",
        "    persist_dir=\"hierarchical_index\"  # Directory name for storing the index\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "gather": {
          "logged": 1705025817121
        },
        "id": "PObvkaYCx53G",
        "outputId": "4a7d11fe-3cd6-4c37-b779-3db9a4306846"
      },
      "outputs": [],
      "source": [
        "from llama_index.retrievers.auto_merging_retriever import AutoMergingRetriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "\n",
        "# Initialize an AutoMergingRetriever with the hierarchical index.\n",
        "# AutoMergingRetriever is used for retrieving data from an index by automatically merging results \n",
        "# from multiple queries or sources. It's particularly useful for complex data structures like a hierarchical index.\n",
        "# Parameters:\n",
        "#   hierarchical_index.as_retriever(similarity_top_k=3): Converts the hierarchical index into a retriever\n",
        "#                                                        configured to find the top 3 most similar entries.\n",
        "#   storage_context: Specifies the storage context from the hierarchical index for data management.\n",
        "#   verbose: Enables verbose output, providing more detailed information during retrieval operations.\n",
        "retriever = AutoMergingRetriever(\n",
        "    hierarchical_index.as_retriever(similarity_top_k=3),\n",
        "    storage_context=hierarchical_index.storage_context,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Create a RetrieverQueryEngine using the AutoMergingRetriever.\n",
        "# RetrieverQueryEngine is a query engine that uses a specified retriever for querying the indexed data.\n",
        "# It allows for complex query operations, especially in conjunction with retrievers like AutoMergingRetriever.\n",
        "amretriever_query_engine = RetrieverQueryEngine.from_args(retriever)\n",
        "\n",
        "# Perform a query using the AMRetriever query engine.\n",
        "# The query is about the influence of camelid genetics on wool quality. The query engine processes this\n",
        "# input and retrieves the most relevant entries from the index.\n",
        "amretriever_response = amretriever_query_engine.query(\"How do camelid genetics influence wool quality?\")\n",
        "\n",
        "# Retrieve the response from the query.\n",
        "# The response contains the results of the query as determined by the query engine, potentially offering\n",
        "# valuable insights into the queried topic.\n",
        "response = amretriever_response.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1705025832194
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Camelid genetics play a significant role in determining wool quality. While there is still much to be understood in this field, recent advancements in genetic understanding have shed light on the genetic mechanisms that regulate economically important fiber traits in South American camelids. Mutations responsible for some monogenic or oligogenic traits have been identified, allowing for molecular testing to assist breeding decisions. Additionally, the development of a 76K SNPs array for the alpaca has facilitated the identification of genes affecting more complex traits through genome-wide association studies. These advancements in genomics and the discovery of genetic variants are expected to contribute to the improvement of wool quality in camelids.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a09oy-4V2qbH"
      },
      "source": [
        "# Evaluating RAG Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Generating a test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1705035897955
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 10%|█         | 1/10 [00:38<05:43, 38.18s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 30%|███       | 3/10 [01:33<03:32, 30.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 6/10 [02:36<01:38, 24.69s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 10/10 [03:46<00:00, 20.82s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "15it [04:22, 14.10s/it]                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "21it [05:39, 13.54s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "28it [07:12, 15.46s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n"
          ]
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "import random\n",
        "\n",
        "# Initialize a TestsetGenerator using its default settings.\n",
        "# TestsetGenerator is used for generating test datasets, typically for model evaluation or testing.\n",
        "# The 'from_default' method sets up the generator with default configurations.\n",
        "testsetgenerator = TestsetGenerator.from_default()\n",
        "\n",
        "# Specify the sample size for the source documents.\n",
        "# This determines how many documents will be randomly selected from the source documents.\n",
        "sample_size = 10\n",
        "\n",
        "# Define the number of questions to be included in the test set.\n",
        "# This will set how many test cases or questions the test set will contain.\n",
        "num_questions = 10\n",
        "\n",
        "# Generate a test dataset from a random sample of source documents.\n",
        "# 'random.sample' is used to randomly select a subset of documents from the source.\n",
        "# The test set is then generated based on these documents.\n",
        "# Parameters:\n",
        "#   random.sample(source_docs, sample_size): A randomly selected subset of source documents.\n",
        "#   test_size: The number of questions or test cases to generate in the test set.\n",
        "testset = testsetgenerator.generate(\n",
        "    random.sample(source_docs, sample_size),  # Randomly selected documents\n",
        "    test_size=num_questions                    # Number of questions in the test set\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Minor cleanup and reformatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "gather": {
          "logged": 1705044441447
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>ground_truth_context</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>question_type</th>\n",
              "      <th>episode_done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the role of the melanocortin 1receptor...</td>\n",
              "      <td>However, color inheritance in domestic South ...</td>\n",
              "      <td>The role of the melanocortin 1receptor MC1R in...</td>\n",
              "      <td>simple</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What specific integumental characteristics con...</td>\n",
              "      <td>SACs have developed several special integumen...</td>\n",
              "      <td>The specific integumental characteristics that...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are the possible longterm effects of shea...</td>\n",
              "      <td>She concluded that shearing alpaca in winter ...</td>\n",
              "      <td>The possible longterm effects of shearing alpa...</td>\n",
              "      <td>multicontext</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the role of hair in thermoregulation i...</td>\n",
              "      <td>The specic integumental characteristics of SA...</td>\n",
              "      <td>The role of hair in thermoregulation in South ...</td>\n",
              "      <td>simple</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the effect of ASIP mutations on melani...</td>\n",
              "      <td>However, if the agouti signaling protein ASIP...</td>\n",
              "      <td>The effect of ASIP mutations on melanin produc...</td>\n",
              "      <td>simple</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What role does MC1R play in coat color regulat...</td>\n",
              "      <td>For example, the mating between two white ani...</td>\n",
              "      <td>MC1R plays a role in coat color regulation in ...</td>\n",
              "      <td>multicontext</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What factors contribute to the higher stress l...</td>\n",
              "      <td>We found a strong positive correlation betwee...</td>\n",
              "      <td>The factors that contribute to higher stress l...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>In contrast, what factors contribute to lower ...</td>\n",
              "      <td>We interpret this as indicating that some ind...</td>\n",
              "      <td>The factors that contribute to lower stress le...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What is the role of the melanocortin 1receptor...   \n",
              "1  What specific integumental characteristics con...   \n",
              "2  What are the possible longterm effects of shea...   \n",
              "3  What is the role of hair in thermoregulation i...   \n",
              "4  What is the effect of ASIP mutations on melani...   \n",
              "5  What role does MC1R play in coat color regulat...   \n",
              "6  What factors contribute to the higher stress l...   \n",
              "7  In contrast, what factors contribute to lower ...   \n",
              "\n",
              "                                ground_truth_context  \\\n",
              "0   However, color inheritance in domestic South ...   \n",
              "1   SACs have developed several special integumen...   \n",
              "2   She concluded that shearing alpaca in winter ...   \n",
              "3   The specic integumental characteristics of SA...   \n",
              "4   However, if the agouti signaling protein ASIP...   \n",
              "5   For example, the mating between two white ani...   \n",
              "6   We found a strong positive correlation betwee...   \n",
              "7   We interpret this as indicating that some ind...   \n",
              "\n",
              "                                        ground_truth question_type  \\\n",
              "0  The role of the melanocortin 1receptor MC1R in...        simple   \n",
              "1  The specific integumental characteristics that...     reasoning   \n",
              "2  The possible longterm effects of shearing alpa...  multicontext   \n",
              "3  The role of hair in thermoregulation in South ...        simple   \n",
              "4  The effect of ASIP mutations on melanin produc...        simple   \n",
              "5  MC1R plays a role in coat color regulation in ...  multicontext   \n",
              "6  The factors that contribute to higher stress l...     reasoning   \n",
              "7  The factors that contribute to lower stress le...     reasoning   \n",
              "\n",
              "  episode_done  \n",
              "0         True  \n",
              "1         True  \n",
              "2         True  \n",
              "3         True  \n",
              "4         True  \n",
              "5         True  \n",
              "6        False  \n",
              "7         True  "
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re \n",
        "\n",
        "test_df = testset.to_pandas()\n",
        "# Define the regex pattern to match any character that is NOT a letter, a number, '.', ',', or '?'\n",
        "pattern = r\"[^a-zA-Z0-9.,? ]\"\n",
        "\n",
        "# Define a function to replace special characters in a string\n",
        "def remove_special_chars(s):\n",
        "    return re.sub(pattern, '', str(s))\n",
        "\n",
        "# Apply the function to each cell in the DataFrame\n",
        "test_df = test_df.applymap(remove_special_chars)\n",
        "\n",
        "\n",
        "test_questions = test_df['question'].values.tolist()\n",
        "test_answers = [[item] for item in test_df['ground_truth'].values.tolist()]\n",
        "\n",
        "test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Running the evaluation for the 3 RAG methods across 6 metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "gather": {
          "logged": 1705039418250
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from ragas.metrics import (\n",
        "    faithfulness, \n",
        "    answer_relevancy, \n",
        "    context_precision, \n",
        "    context_recall, \n",
        "    answer_similarity, \n",
        "    answer_correctness\n",
        ")\n",
        "from ragas.llama_index import evaluate\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# List of evaluation metrics functions to be used.\n",
        "metrics = [\n",
        "    faithfulness,           # Evaluates faithfulness of the response to the source material.\n",
        "    answer_relevancy,       # Assesses relevance of the response to the query.\n",
        "    context_precision,      # Measures precision of the context in the response.\n",
        "    context_recall,         # Measures recall of the context in the response.\n",
        "    answer_correctness,     # Checks correctness of the answer.\n",
        "    answer_similarity,      # Evaluates similarity of the answer to a reference answer.\n",
        "]\n",
        "\n",
        "# A list to collect individual result DataFrames.\n",
        "results_list = []\n",
        "\n",
        "# A list of tuples, each containing a query engine and its corresponding technique name.\n",
        "indices = [\n",
        "    (baseline_query_engine, 'chunks_with_overlap'),\n",
        "    (sentence_query_engine, 'sentence_window'),\n",
        "    (amretriever_query_engine, 'hierarchical_automerge')\n",
        "]\n",
        "\n",
        "# Iterate over each query engine and technique pair.\n",
        "for query_engine, technique in indices:\n",
        "    # Evaluate the query engine.\n",
        "    result = evaluate(query_engine, metrics, test_questions, test_answers)\n",
        "\n",
        "    # Add a 'technique' column to the result DataFrame.\n",
        "    result['technique'] = technique\n",
        "\n",
        "    # Add the result DataFrame to the results list.\n",
        "    results_list.append(result)\n",
        "\n",
        "    # Sleep to handle rate limits.\n",
        "    time.sleep(60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "gather": {
          "logged": 1705039375583
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>answer_correctness</th>\n",
              "      <th>answer_similarity</th>\n",
              "      <th>technique</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.718750</td>\n",
              "      <td>0.864502</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.9250</td>\n",
              "      <td>0.557648</td>\n",
              "      <td>0.964971</td>\n",
              "      <td>chunks_with_overlap</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.852083</td>\n",
              "      <td>0.990066</td>\n",
              "      <td>0.802083</td>\n",
              "      <td>0.8875</td>\n",
              "      <td>0.620310</td>\n",
              "      <td>0.977860</td>\n",
              "      <td>sentence_window</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.843750</td>\n",
              "      <td>0.962274</td>\n",
              "      <td>0.937500</td>\n",
              "      <td>0.8750</td>\n",
              "      <td>0.557285</td>\n",
              "      <td>0.962493</td>\n",
              "      <td>hierarchical_automerge</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   faithfulness  answer_relevancy  context_precision  context_recall  \\\n",
              "0      0.718750          0.864502           0.625000          0.9250   \n",
              "1      0.852083          0.990066           0.802083          0.8875   \n",
              "2      0.843750          0.962274           0.937500          0.8750   \n",
              "\n",
              "   answer_correctness  answer_similarity               technique  \n",
              "0            0.557648           0.964971     chunks_with_overlap  \n",
              "1            0.620310           0.977860         sentence_window  \n",
              "2            0.557285           0.962493  hierarchical_automerge  "
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert each Result object's items to a dictionary and collect them in a list\n",
        "dict_list = [dict(result.items()) for result in results_list]\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "results_df = pd.DataFrame(dict_list)\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Tracking RAG Evaluation Results on MLFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%pip install mlflow azureml-mlflow -U -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "gather": {
          "logged": 1705046683236
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace\n",
        "import mlflow\n",
        "\n",
        "# Load the Azure ML workspace configuration\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Set the MLflow tracking URI\n",
        "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
        "\n",
        "# Set the MLflow experiment name\n",
        "mlflow.set_experiment(\"advanced_rag_eval2\")\n",
        "\n",
        "# Assuming 'technique' is the column name in results_df that stores the technique name\n",
        "# And other columns in results_df are the metrics you want to log\n",
        "for index, row in results_df.iterrows():\n",
        "    # Start a new MLflow run\n",
        "    with mlflow.start_run(run_name=f\"{row['technique']}\"):  # Use 'technique' column to name the run\n",
        "        # Log each metric in the row\n",
        "        for metric in row.index:\n",
        "            if metric != 'technique':  # Exclude the 'technique' column from metrics\n",
        "                mlflow.log_metric(metric, row[metric])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNA+DPd2MtegQk7gQ5AOmkD",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
